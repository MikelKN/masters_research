{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Loading the train dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport re\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading the train dataset\ntrain_df = pd.read_csv(\"../input/multiclass-train/my_train_set.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sub_num(text):\n    #substitutinging numbers\n    text = re.sub('[0-9]+', 'numbr', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['clean_text'] = train_df['clean_text'].apply(str).apply(lambda x: sub_num(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['review_len'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Bi-directional Long Short-Term Memory**\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Try this next\nhttps://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/\nhttps://www.kaggle.com/vikashrajluhaniwal/sentiment-analysis-using-lstm-in-keras-93-acc\nhttps://www.kaggle.com/shubhams9k96/sequence-classification-85-acc\nhttps://www.kaggle.com/eriche523/bidirectional-lstm-in-keras\nhttps://www.kaggle.com/imdevskp/imdb-review-classification-lstm-gru-cnn-glove"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Building Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nimport keras.backend as K\nfrom keras.layers import Dense, Dropout, Activation, Embedding\nfrom keras.layers import LSTM, Bidirectional\nfrom keras.models import Sequential\nfrom keras.preprocessing import sequence\nfrom collections import Counter\nfrom keras.utils import np_utils\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\n\n\nimport decimal\nimport tensorflow as tf\nimport re\nfrom keras.models import load_model\n\n\navg = 'weighted'\nnum_classes = 4\nROUND_PREC = 1\nNUM_EPOCHS = 200  # Will use early stopping to determine num. of epochs\nencoder = LabelEncoder()\nplot_labels = []\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n\nNotes on \n# Keras.Callback:\n- Keras supports the early stopping of training via a callback called EarlyStopping.This callback allows you to specify the performance measure to monitor, the trigger, and once triggered, it will stop the training process.\n- The “monitor” allows you to specify the performance measure to monitor in order to end training.\n- Often, the first sign of no further improvement may not be the best time to stop training. This is because the model may coast into a plateau of no improvement or even get slightly worse before getting much better.We can account for this by adding a delay to the trigger in terms of the number of epochs on which we would like to see no improvement. This can be done by setting the “patience” argument.\n\n# Keras.Checkpointing:\nThe EarlyStopping callback will stop training once triggered, but the model at the end of training may not be the model with best performance on the validation dataset.\n- An additional callback is required that will save the best model observed during training for later use. This is the ModelCheckpoint callback. \n> tf.keras.callbacks.ModelCheckpoint  when used, helps to periodically save your model during training.\n- The callback will save the model to file, which requires that a path and filename be specified via the first argument \n> mc = ModelCheckpoint('best_model.h5')\n- The preferred loss function to be monitored can be specified via the monitor argument. Also,  we are interested in only the very best model observed during training and this can be achieved by setting the SAVE_BEST_ONLY = TRUE.\n>  mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n- You can save your model by calling the save() function on the model and specifying the filename.\nhttps://machinelearningmastery.com/save-load-keras-deep-learning-models/\n\n# Keras.Callback. on_epoch_end\nhttps://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback\n\n# Keras.CSVLogger\nCallback that streams epoch results to a CSV file\n\n# Chckepoint example\n\nhttps://www.kaggle.com/vikramtiwari/tf2-tutorials-keras-save-and-restore-models"},{"metadata":{},"cell_type":"markdown","source":"# Task 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\ntop_words = 8000\ntokenizer = Tokenizer(num_words=top_words)\ntokenizer.fit_on_texts(train_df['clean_text'])\n\nX_train = tokenizer.texts_to_sequences(train_df['clean_text'])\n#X_test = tokenizer.texts_to_sequences(sentences_test)\n\nvocab_size = len(tokenizer.word_index) + 1 \n\nprint(vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Trying the use the 3 gram to see if my model perfromance increases\n# from sklearn.feature_extraction.text import TfidfVectorizer\n\n# vectorizer = TfidfVectorizer(ngram_range=(5,3)) \n# vectorized = vectorizer.fit_transform(train_df['clean_text']).toarray()\n# vectorized.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.Text[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.clean_text[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(vectorized[2].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n#X = sequence.pad_sequences(train_vector, maxlen=550)  # padding the training set\nX = pad_sequences(X_train, padding='post', maxlen=500)\npredictor_set = train_df['ratings']\nY = keras.utils.np_utils.to_categorical(predictor_set)\n\n\nprint('\\n\\t\\t\\t\\t.............................Task 1 begins ................................')\n\nprint('\\nChecking the different ratios of ratings in the predictor set: {} '.format(Counter(predictor_set)))\n\n#Checking the different labels\nencoder.fit(predictor_set)\nlabels = []\nfor i in range(0, 4):\n    labels.append(encoder.inverse_transform([i])[0])\nprint('\\nThe different lables are: ', labels)\n\n#Splitting the data into training and validation set( 70 : 30)\nprint('\\nSplitting our dataset into training and validatin set...')\ntrain_x, x_val, train_y, y_val = train_test_split(X, Y, stratify = Y, test_size = 0.2, random_state = 42)\nprint('\\nshape of x_val:', x_val.shape)\nprint('shape of y_val:', y_val.shape)\n\nprint('\\n\\t\\t\\t\\t..............................Task 1 compleeted ............................')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initializing our TPU"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_seq_len = 500\nlstm_dim  = 220 #256 better, 150 best\nbatch_size = 200  #best is 200 lstm_dim =200 (69,)\ndropout = 0.6 #thi is the best dropout  \nemb_dim  = 100    #try 64, 32 was 100\nlearning_rate = 0.0002 #changed form 0.01 then 0.001\nnum_classes = 4\nloss = 'categorical_crossentropy'\nmetric = 'categorical_accuracy'\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import SpatialDropout1D\ndef build_model():#Training\n    print('Training...')\n\n    model = Sequential()\n    \n    #model.add(Embedding(vocab_size, emb_dim))\n    model.add(Embedding(top_words, emb_dim, input_length=max_seq_len))\n    \n    #model.add(SpatialDropout1D(0.7))\n\n    model.add(Bidirectional(LSTM(lstm_dim, dropout=dropout, recurrent_dropout=dropout)))\n    #model.add(LSTM(lstm_dim, dropout=dropout, recurrent_dropout=dropout))\n\n    model.add(Dense(num_classes))\n\n    model.add(Activation('softmax'))\n\n    return (model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    model = build_model()\n     \n#metric = 'categorical_accuracy'eval_metrics = ['loss', 'categorical_accuracy', 'val_loss', 'val_categorical_accuracy']\ncur_optimizer = keras.optimizers.Adam(lr=learning_rate)\nmodel.compile(loss='categorical_crossentropy', optimizer=cur_optimizer, metrics=['categorical_accuracy']) #we will be monitoring the categorical accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Call backs for early stopping (https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/)\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min')\n\ncheckpoint_path = \"weights_base.best.hdf5\"  #Changed this\ncheckpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='min', period=1)\n\nhistory = model.fit(train_x, train_y, batch_size=batch_size, \n                        epochs=200, validation_split = 0.2, callbacks=[early_stop,checkpoint])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\ndef plot_history(history):\n    acc = history.history['categorical_accuracy']\n    val_acc = history.history['val_categorical_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=history.history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Determing the different classes in our dataset\nimport itertools\nnum_classes = 4\n\ndef diff_classes (num_classes):\n    for i in range(0, num_classes):\n        plot_labels.append(i)\n    return(plot_labels)\n\n\nplot_classes = diff_classes(num_classes)\n\ndef compute_confusion_matrix(cm, labels , title='plot confusion matrix', cmap=plt.cm.Blues):\n        \n        fig, ax = plt.subplots(figsize=(5,5))\n\n        plt.imshow(cm, cmap=cmap )\n        plt.title(title)\n        tick_marks = np.arange(len(labels))\n        plt.xticks(tick_marks, labels, rotation=45)\n        plt.yticks(tick_marks, labels)\n\n        fmt = 'd'\n        thresh = cm.max() / 2.\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(j, i, format(cm[i, j], fmt),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n        plt.ylabel('Actual label')\n        plt.xlabel('Predicted label')\n        plt.tight_layout()\n        plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\ndef model_evaluate(validation_data = ()):\n    fs = []\n    x_val, y_val = validation_data\n\n    y_pred = np.asarray(model.predict(x_val)) # getting for one epoch\n    y_gold = y_val\n\n    #assert len(y_pred) == len(y_gold)\n\n    y_pred_final, y_gold_final = ([] for _ in range(2))\n\n    for gold, pred in zip(y_gold, y_pred):\n        max_i_pred = (np.argsort(pred)[::-1])[0]  #then sorting the result per epoch and fininding the max_pred for that epoch\n        max_pred_score = pred[max_i_pred]      #fininding the score for the prediction\n        max_i_gold = (np.argsort(gold)[::-1])[0]\n        max_gold_score = gold[max_i_gold]\n\n        y_pred_final.append(max_i_pred)\n\n        y_gold_final.append(max_i_gold)\n        \n#     #his will help see the mispredictions   \n#     pred_data = pd.DataFrame(y_pred)\n#     pred_data.to_csv(\"predictions.csv\", header=True, index=False)\n\n\n    fs = f1_score(y_gold_final, y_pred_final, average=None)\n    #fs.append(';'.join(str(round(100*x, ROUND_PREC)) for x in fs.tolist()))\n\n    #defining and plotting our confusion matrix\n\n    print('The confusion matrix: ')\n    cnf_matrix = confusion_matrix(y_gold_final, y_pred_final)\n    plt.figure()\n\n    compute_confusion_matrix(cnf_matrix, labels=labels,\n                          title='plot_Confusion_matrix')\n\n\n    #Gettign the classification report\n    print('The classification report: ')\n    target_names = ['class_' + i for i in [str(i) for i in list(plot_classes)]]\n    print(classification_report(y_gold_final, y_pred_final, target_names=target_names))\n\n\n    print('metrics used are:\\n f1_score = {}'.format(fs))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_evaluate(validation_data = (x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_epoch_metrics = []\n\n# Determine if best loss is last epoch or the one before it\n#epoch number is 0based\nbest_epoch_num = (len(history['val_loss'])-2)  if (history['val_loss'][len(history['val_loss'])-1] > history['val_loss'][len(history['val_loss'])-2]) else (len(history['val_loss'])-1) \nbest_epoch_metrics.append(str(best_epoch_num))\nprint ('The best Epoch:', best_epoch_num )\n\n\n'''#saving the best metrics for the best epoch\nfor metric in eval_metrics:\n    metric_vals = history[metric]\n    best_epoch_metrics.append(str(round(float(100*metric_vals[best_epoch_num]), ROUND_PREC)))\n\nprint(\"The best epoch metrics are:\", best_epoch_metrics)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#stop here for now#","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TESTING ON TEST_SET"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model\ntest_model = load_model('weights_base.best.hdf5')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the test set initially saved\ntest_df = pd.read_csv(\"../input/testing/my_test_set.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['clean_text'] = test_df['clean_text'].apply(str).apply(lambda x: sub_num(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['review_len'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_features = test_df['clean_text']\n# test_predictor = test_df['ratings']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.fit_on_texts(test_df['clean_text'])\n\ntest_data = tokenizer.texts_to_sequences(test_df['clean_text'])\n#X_test = tokenizer.texts_to_sequences(sentences_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#padding the test_features\ntest_features = pad_sequences(test_data, padding='post', maxlen=500)\ntest_labels = keras.utils.np_utils.to_categorical(test_df['ratings'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_test_evaluate(validation_data = ()):\n    fs = []\n    x_val, y_val = validation_data\n    ##################################\n    y_pred = np.asarray(model.predict(x_val))\n    y_gold = y_val\n    #assert len(y_pred) == len(y_gold)\n\n    y_pred_final, y_gold_final = ([] for _ in range(2))\n\n    for gold, pred in zip(y_gold, y_pred):\n        max_i_pred = (np.argsort(pred)[::-1])[0]  #then sorting the result per epoch and fininding the max_pred for that epoch\n        max_pred_score = pred[max_i_pred]      #fininding the score for the prediction\n        max_i_gold = (np.argsort(gold)[::-1])[0]\n        max_gold_score = gold[max_i_gold]\n\n        y_pred_final.append(max_i_pred)\n        y_gold_final.append(max_i_gold)\n        \n        #his will help see the mispredictions   \n    pred_test_data = pd.DataFrame(y_pred)\n    pred_test_data.to_csv(\"predictions_test.csv\", header=True, index=False)\n\n\n    fs = f1_score(y_gold_final, y_pred_final, average=None)\n   #fs.append(';'.join(str(round(100*x, ROUND_PREC)) for x in fs.tolist()))\n\n    #defining and plotting our confusion matrix\n\n    print('The confusion matrix: ')\n    cnf_matrix = confusion_matrix(y_gold_final, y_pred_final)\n    plt.figure()\n\n    compute_confusion_matrix(cnf_matrix, labels=labels,\n                          title='plot_Confusion_matrix')\n\n\n    #Gettign the classification report\n    print('The classification report: ')\n    target_names = ['class_' + i for i in [str(i) for i in list(plot_classes)]]\n    print(classification_report(y_gold_final, y_pred_final, target_names=target_names))\n\n    print('metrics used are:\\n f1_score = {}'.format(fs))\n    return pred_test_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_test_evaluate(validation_data = (test_features, test_labels));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load the predicted set and calculate the misprediction rates\n  ''' y_pred_scores = model.predict(x_val)\n\n    y_pred_labels, y_pred = ([] for _ in range(2))\n\n    for i in range(len(y_pred_scores)):\n        cur_pred = y_pred_scores[i]\n        max_i = np.argsort(cur_pred)[::-1][0]\n        max_score = cur_pred[max_i]\n\n        y_pred.append(max_i)\n\n        pred_label = encoder.inverse_transform(max_i)\n        y_pred_labels.append(pred_label)\n\n        y_val = encoder.transform(y_val)\n        assert len(y_pred_labels) == len(y_val)\n\n        pred_data = pd.DataFrame(y_pred)\n        pred_data.to_csv(\"predictions.csv\", header=True, index=False)\n    '''\n    #################################\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load the predicted set\n#y_test_pred = pd.read_csv(\"\")\ntest_df['Predicted_label'] = pred_test_data\ntest_mispredictions = test_df[['clean_text',\"ratings\", 'Predicted_label', 'Originally']][test_labels != pred_test_data]\n\ntest_mispredictions.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"percentage of mispredictions from test_data: {} %\". format((len(test_mispredictions)/len(test_df))*100)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"stop here for now, \nhttps://machinelearningmastery.com/use-keras-deep-learning-models-scikit-learn-python/\nhttps://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/\nhttps://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/\nhttps://towardsdatascience.com/dont-overfit-how-to-prevent-overfitting-in-your-deep-learning-models-63274e552323"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"to read for tomorrow\n\nhttps://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/\n\nhttps://www.kaggle.com/questions-and-answers/92749\n\nhttps://www.kaggle.com/artgor/eda-and-lstm-cnn/notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}