{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport re\nimport matplotlib.pyplot as plt\nfrom string import punctuation\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.preprocessing import sequence\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n#print long strings without truncating in pandas\npd.options.display.max_colwidth = 500\n\nmax_len_bert = 512\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"movie_dataset = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\nclothing_dataset = pd.read_csv(\"../input/womens-ecommerce-clothing-reviews/Womens Clothing E-Commerce Reviews.csv\", index_col = 0)\nfood_dataset = pd.read_csv(\"/kaggle/input/amazon-fine-food-reviews/Reviews.csv\", index_col =[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clothing_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"food_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets take a look at each of these datasets one at a time \n\nMOVIE DATASET:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The shape of the movie dataset is: {}\".format (movie_dataset.shape))\nprint('\\nSome sample datapoint:\\n')\n\nmovie_dataset.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets get how balanced our movie dataset is\nmovie_dataset.sentiment.value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our movie dataset is equally balanced. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Do we have any nans?\n#which columns have nans\nmovie_dataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets convert the sentiments from categorical to numerical features\nle = LabelEncoder()\nmovie_dataset['sentiment'] = le.fit_transform(movie_dataset['sentiment'])\nmovie_dataset.rename(columns = {'review':'Text', 'sentiment':'ratings'}, inplace=True)\n\nmovie_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Women E-ommerce clothing dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The shape of the women dataset is: {}\".format (clothing_dataset.shape))\nprint('\\nSome sample datapoint:\\n')\n\nclothing_dataset.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#WE are interested in only 2 columns in this dataset. So we will drop some of the columns\n\nclothing_dataset.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clothing_dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clothing_dataset.drop([col for col in clothing_dataset.columns if col not in ['Review Text', 'Rating']], axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clothing_dataset.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clothing_dataset.Rating.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Do we have any null/empty cell\n#which columns have nans\nclothing_dataset.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clothing_dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping all the empty/ null cell from the clothing dataset\nclothing_dataset = clothing_dataset[~clothing_dataset['Review Text'].isnull()]\n\nclothing_dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we shall consider all ratings 1, 2 as negative reviews and reviews 4, 5 as positive reviews\n#All reviews with Rating = 3 shall be considered neutrl reviews and for simplicity, shall be dropped.\n\nclothing_reviews=clothing_dataset[clothing_dataset.Rating!=3]\nclothing_reviews[\"ratings\"]= clothing_dataset[\"Rating\"].apply(lambda x: 1 if x > 3  else 0)\nclothing_reviews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clothing_reviews=clothing_reviews.drop(\"Rating\",axis=1)\nclothing_reviews.rename(columns = {'Review Text':'Text'}, inplace=True)\n\nclothing_reviews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clothing_reviews.ratings.value_counts(normalize = True)\n# this dataset is very imbalanced","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style = 'darkgrid')\nax = sns.countplot(x = 'ratings', data = clothing_reviews)\nax.set_title('Ratings distribution for women clothing dataset');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Amazon fine food dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"food_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"food_dataset.drop([col for col in food_dataset.columns if col not in ['Text','Score']], axis=1, inplace=True)\nfood_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"food_dataset.Score.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"food_dataset.Score.value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Since i want to have a binary classification problem with only positive and negative reviews, I will focus mostly on the reviews with 1, 2 as my negative reviews and the reviews with 4, 5 as my positive reviews.For simplicity,  I will dropp all reviews with 3 as i consider them as neutral reviews. *"},{"metadata":{"trusted":true},"cell_type":"code","source":"#we shall consider all ratings 1, 2 as negative reviews and reviews 4, 5 as positive reviews\n\nfood_reviews=food_dataset[food_dataset.Score!=3]\nfood_reviews[\"ratings\"]= food_dataset['Score'].apply(lambda x: 1 if x > 3  else 0)\nfood_reviews=food_reviews.drop(\"Score\",axis=1)\n\nfood_reviews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"food_reviews.ratings.value_counts(normalize = True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"this dataset is highly imbalanced. But we will not worrry about it now. We will have to extract an equal number of positive and negative reviews when we are creating our unified dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets find some empty cell and delete them\nfood_reviews.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"MULTICLASS DATASET CREATION\n\nDescription:\n\nThe main aim here was to create a unified dataset(multiclass_customer_review_data), which contains the movie_dataset, the women_dataset and the yelp_dataset into one dataset.\n\nWe attributed the\n\n--- label '0' to the negative examples of all the 3 dataset.\n\n--- label '1' for the positive examples from the movie_dataset\n\n--- label '2' for the positive examples form the women_dataset\n\n--- label '3' for the positive examples form the food_dataset"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Since we are aiming for a balanced dataset, we are going to creat a balanced dataset for all the 3 datasets we are using, before creating the unified multiclass dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets get all the positive reviewss together\n\npositive_movies_reviews = movie_dataset[movie_dataset.ratings == 1]\n\npositive_clothing_reviews = clothing_reviews[clothing_reviews.ratings == 1]\npositive_clothing_reviews['ratings'].replace(1, 2, inplace = True)\n\npositive_food_reviews = food_reviews[food_reviews.ratings == 1]\npositive_food_reviews['ratings'].replace(1, 3, inplace = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating a new column to keep track of the originals in the new datasets\n\npositive_movies_reviews['Originally'] = 'Movies'\npositive_clothing_reviews['Originally'] = 'Clothing'\npositive_food_reviews['Originally'] = 'Food' \n\npositive_movies_reviews.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we get all the negative reviews together\nnegative_movies_reviews = movie_dataset[movie_dataset.ratings == 0]\nnegative_clothing_reviews = clothing_reviews[clothing_reviews.ratings == 0]\nnegative_food_reviews = food_reviews[food_reviews.ratings == 0]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating a new column to keep track of their original datasets\n\nnegative_movies_reviews['Originally'] = 'Movies'\n\nnegative_clothing_reviews['Originally'] = 'Clothing'\n\nnegative_food_reviews['Originally'] = 'Food'\n\nnegative_movies_reviews.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Creating our dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_dataset = pd.concat([positive_movies_reviews, positive_clothing_reviews, positive_food_reviews], axis = 0, ignore_index = True, sort= False)\npositive_dataset.sample(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dataframes were merged/concatenated correctly. We have no duplicate index.\npositive_dataset.index[positive_dataset.index.duplicated()].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_values(df, feature):\n    total=df.loc[:,feature].value_counts()\n    percentage=round(df.loc[:,feature].value_counts(normalize = True)*100, 2)\n    \n    return pd.concat([total, percentage], axis =1, keys = ['Total', 'Percentage'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_values(positive_dataset, 'ratings')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_dataset = pd.concat([negative_movies_reviews, negative_clothing_reviews, negative_food_reviews], axis=0, ignore_index = True, sort = False)\n\n#negative_dataset.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_dataset.index[negative_dataset.index.duplicated()].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_dataset.index[negative_dataset.index.duplicated()].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"balanced_pos = positive_dataset[positive_dataset['Originally']=='Food'][:65000].append(positive_dataset[positive_dataset['Originally']=='Movies'], ignore_index = True, sort = False)\nbalanced_pos = balanced_pos.append(positive_dataset[positive_dataset['Originally']=='Clothing'], ignore_index = True, sort = False)\nbalanced_pos.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"balanced_pos.index[balanced_pos.index.duplicated()].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Concatenating the positive and negative exxamples to form our multiclass dataset\n\n#Concatenating the positive and the negative training datasets together\n\nmulticlass_dataset = pd.concat([negative_dataset, balanced_pos],ignore_index = True, sort = False)\n#multiclass_dataset.sample(frac=1)#trying to shuffle it a little bit\n\nmulticlass_dataset.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''def shuffle(df, n=1, axis=0):     \n    df = df.copy()\n    for _ in range(n):\n        df.apply(np.random.shuffle, axis=axis)\n    return df'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Shuffle Pandas data frame\nimport sklearn.utils\nmulticlass_dataset = sklearn.utils.shuffle(multiclass_dataset)\nmulticlass_dataset = multiclass_dataset.reset_index(drop= True)\nmulticlass_dataset.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multiclass_dataset.ratings.value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see how ur unified dataset is distributed"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualisation library\nfrom plotly.offline import iplot\nimport plotly.graph_objs as go\n#import chart_studio.plotly as py\nimport plotly.figure_factory as ff\nfrom plotly.offline import iplot\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\n\n\nmulticlass_dataset['ratings'].value_counts(normalize=True).iplot(kind='bar',\n                                                      yTitle='Percentages', \n                                                      linecolor='black', \n                                                      opacity=0.9,\n                                                      color='blue',\n                                                      theme='pearl',\n                                                      bargap=0.6,\n                                                      gridcolor='white',\n                                                     \n                                                      title='Distribution of Ratings column in the multiclass dataset')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multiclass_dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Text Preprocessing - preparation for Training with BILSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"multiclass_lstm = multiclass_dataset.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import FreqDist","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing\nhttps://machinelearningmastery.com/prepare-text-data-deep-learning-keras/\n\nText preprocessing:\nWe will perfrom the following operations on our multicalss dataset.\n\n- Removing website/URLs links\n- Removing All emails\n- Removing or transforming all emogies\n- Removing the words with numeric digits\n- Transforming all numbrs to \"Num\"\n- Removing all white spaces\n- Converting text to lower case\nLower casing is a common text preprocessing technique. The idea is to convert the input text into same casing format so that 'text', 'Text' and 'TEXT' are treated the same way.\n- Removing stop words\n- Performing Lemmatization"},{"metadata":{"trusted":true},"cell_type":"code","source":"err1 = multiclass_dataset['Text'].str.extractall(\"(&amp)\")\nerr2 = multiclass_dataset['Text'].str.extractall(\"(\\xa0)\")\n\nprint('with &amp',len(err1[~err1.isna()]))\nprint('with (\\xa0)',len(err2[~err2.isna()]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multiclass_dataset['Text'] = multiclass_dataset['Text'].str.replace('(&amp)','')\nmulticlass_dataset['Text'] = multiclass_dataset['Text'].str.replace('(\\xa0)','')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/shirellamosi/sentiment-analysis-nlp\n#https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n\nSTOPWORDS = set(stopwords.words('english'))\nfrom string import punctuation\nfrom bs4 import BeautifulSoup\n\ndef clean_text(text):\n    '''\n    Preprocess/normalize text\n    '''\n    \n    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n    email_regex = '(^[a-z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)'\n    happy_emoticon_regex = ': *\\)'\n    sad_emoticon_regex = ': *\\('\n    emojis =\"ğŸ•ğŸµğŸ˜‘ğŸ˜¢ğŸ¶ï¸ğŸ˜œğŸ˜ğŸ‘ŠğŸ¤ªğŸ˜ğŸ˜ğŸ’–ğŸ’µğŸ‘ğŸ˜€ğŸ˜‚ğŸ”¥â­ğŸ¤¯ğŸ˜„ğŸ¤ªğŸ»ğŸ’¥ğŸ˜‹ğŸ‘ğŸ˜±ğŸšŒá´µÍğŸŒŸğŸ˜ŠğŸ˜³ğŸ˜§ğŸ•ğŸ™€ğŸ˜ğŸ˜•ğŸ‘ğŸ˜®ğŸ˜ƒğŸ˜˜ğŸ’©ğŸ’¯â›½ğŸš„ğŸ˜–ğŸ¼ğŸš²ğŸ˜ŸğŸ˜ˆğŸ’ªğŸ™ğŸ¯ğŸŒ¹ğŸ˜‡ğŸ’”ğŸ˜¡ğŸ‘ŒğŸ™„ğŸ˜ ğŸ˜‰ğŸ˜¤â›ºğŸ™‚ğŸ˜ğŸ¾ğŸ‰ğŸ˜ğŸ¾ğŸ˜…ğŸ˜­ğŸ‘»ğŸ˜¥ğŸ˜”ğŸ˜“ğŸ½ğŸ†ğŸ»ğŸ½ğŸ¶ğŸŒºğŸ¤”ğŸ˜ªğŸ°ğŸ‡ğŸ±ğŸ™†ğŸ˜¨ğŸ™ƒğŸ’•ğŸ’—ğŸ’šğŸ™ˆğŸ˜´ğŸ¿ğŸ¤—ğŸ‡ºğŸ‡¸â¤µğŸ†ğŸƒğŸ˜©ğŸ‘®ğŸ’™ğŸ¾ğŸ•ğŸ˜†ğŸŒ ğŸŸğŸ’«ğŸ’°ğŸ’ğŸ–ğŸ™…â›²ğŸ°ğŸ¤ğŸ‘†ğŸ™ŒğŸ’›ğŸ™ğŸ‘€ğŸ™ŠğŸ™‰ğŸš¬ğŸ¤“ğŸ˜µğŸ˜’ÍğŸ†•ğŸ‘…ğŸ‘¥ğŸ‘„ğŸ”„ğŸ”¤ğŸ‘‰ğŸ‘¤ğŸ‘¶ğŸ‘²ğŸ”›ğŸ“ğŸ˜£âºğŸ˜ŒğŸ¤‘ğŸŒğŸ˜¯ğŸ˜²ğŸ’ğŸš“ğŸ””ğŸ“šğŸ€ğŸ‘ğŸ’¤ğŸ‡ğŸ¡â”â‰ğŸ‘ ã€‹ğŸ‡¹ğŸ‡¼ğŸŒ¸ğŸŒğŸ²ğŸ˜›ğŸ’‹ğŸ’€ğŸ„ğŸ’œğŸ¤¢ÙÙğŸ—‘ğŸ’ƒğŸ“£ğŸ‘¿à¼¼ã¤à¼½ğŸ˜°ğŸ¤£ğŸğŸ…ğŸºğŸµğŸŒÍŸğŸ¤¡ğŸ¤¥ğŸ˜¬ğŸ¤§ğŸš€ğŸ¤´ğŸ˜ğŸ’¨ğŸˆğŸ˜ºğŸŒâá»‡ğŸ”ğŸ®ğŸğŸ†ğŸ‘ğŸŒ®ğŸŒ¯ğŸ¤¦ğŸ€ğŸ˜«ğŸ¤¤ğŸ¼ğŸ•ºğŸ¸ğŸ¥‚ğŸ—½ğŸ‡ğŸŠğŸ†˜ğŸ¤ ğŸ‘©ğŸ–’ğŸšªğŸ‡«ğŸ‡·ğŸ‡©ğŸ‡ªğŸ˜·ğŸ‡¨ğŸ‡¦ğŸŒğŸ“ºğŸ‹ğŸ’˜ğŸ’“ğŸ’ğŸŒ‹ğŸŒ„ğŸŒ…ğŸ‘ºğŸ·ğŸš¶ğŸ¤˜Í¦ğŸ’¸ğŸ‘‚ğŸ‘ƒğŸ«ğŸš¢ğŸš‚ğŸƒğŸ‘½ğŸ˜™ğŸ¾ğŸ‘¹âŒğŸ’â›¸ğŸ„ğŸ€ğŸš‘ğŸ¤·ğŸ¤™ğŸ’ğŸˆï·»ğŸ¦„ğŸš—ğŸ³ğŸ‘‡â›·ğŸ‘‹ğŸ¦ŠğŸ½ğŸ»ğŸ¹â›“ğŸ¹ğŸ·ğŸ¦†â™¾ğŸ¸ğŸ¤•ğŸ¤’â›‘ğŸğŸğŸ¦ğŸ™‹ğŸ˜¶ğŸ”«ğŸ‘ğŸ’²ğŸ—¯ğŸ‘‘ğŸš¿ğŸ’¡ğŸ˜¦ğŸğŸ‡°ğŸ‡µğŸ‘¾ğŸ„ğŸˆğŸ”¨ğŸğŸ¤ğŸ¸ğŸ’ŸğŸ°ğŸŒğŸ›³ğŸ­ğŸ‘£ğŸ‰ğŸ’­ğŸ¥ğŸ´ğŸ‘¨ğŸ¤³ğŸ¦ğŸ©ğŸ˜—ğŸ‚ğŸ‘³ğŸ—ğŸ•‰ğŸ²ğŸ’ğŸ‘â°ğŸ’ŠğŸŒ¤ğŸŠğŸ”¹ğŸ¤šğŸğ‘·ğŸ‚ğŸ’…ğŸ’¢ğŸ’’ğŸš´ğŸ–•ğŸ–¤ğŸ¥˜ğŸ“ğŸ‘ˆâ•ğŸš«ğŸ¨ğŸŒ‘ğŸ»ğŸ¤–ğŸğŸ˜¼ğŸ•·ğŸ‘¼ğŸ“‰ğŸŸğŸ¦ğŸŒˆğŸ”­ã€ŠğŸŠğŸğŸ¦ğŸ¡ğŸ’³á¼±ğŸ™‡ğŸ¥œğŸ”¼\"\n\n    #number_regex = 'num'\n    white_space_regex = '\\s+'\n    extra_white_space_regex = '^\\s+|\\s+$'\n    lemmatizer = WordNetLemmatizer()\n    \n    text = text.lower()  #lower casing our text\n    \n    words = re.split('\\s+', text)\n    \n    for w in range(len(words)):\n       if re.match(email_regex, words[w]): \n           words[w] = ' email ' #converting all emails \n       elif re.match(url_regex, words[w]):\n           words[w] = ' url '#converting all urls\n    \n    #removing english stopwords\n    word = [word for word in text.split(\" \") if word not in STOPWORDS] # removing stop words\n    \n    #words = [word for word in text.split(\" \") if word not in punctuation]\n    \n    #lemmatization\n    words = [lemmatizer.lemmatize(word) for word in words]\n    \n    text = ' '.join(words)\n    \n    #Emogies\n    '''text = re.sub(happy_emoticon_regex, ' happyemot ', text)\n    text = re.sub(sad_emoticon_regex, ' sademot ', text)\n    '''\n    for emoji in emojis:\n        text = text.replace(emoji, '')\n    \n    text = BeautifulSoup(text, 'lxml').get_text()   # removing html tags\n        \n    text = re.sub('\\*+', '\\*', text) # reduce scurbbing marker to one occurrence\n    \n    text = re.sub('\\<br />', ' ', text)\n    \n    #Spacings\n    text = re.sub(white_space_regex, ' ', text)\n    text = re.sub(extra_white_space_regex, '', text)\n    \n\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the cleaning function to our datasets\nmulticlass_lstm['clean_text'] = multiclass_lstm['Text'].apply(str).apply(lambda x: clean_text(x))\nmulticlass_lstm.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will not remove numbers form my text. I think this nubers may be useful in training my model to recognise reviews. For example, reviews with body measurments mmay indicate that we are dealing with a clothing review."},{"metadata":{},"cell_type":"markdown","source":"# Removing all punctuations"},{"metadata":{"trusted":true},"cell_type":"code","source":"def punctuation_removal(messy_str):\n    clean_list = [char for char in messy_str if char not in punctuation]\n    clean_str = ''.join(clean_list)\n    return clean_str","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multiclass_lstm['clean_text'] = multiclass_lstm['clean_text'].apply(str).apply(lambda x: punctuation_removal(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multiclass_lstm.sample(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decontract"},{"metadata":{"trusted":true},"cell_type":"code","source":"def decontract(text):\n    text = re.sub(r\"won\\'t\", \"will not\", text)\n    text = re.sub(r\"can\\'t\", \"can not\", text)\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multiclass_lstm['clean_text'] = multiclass_lstm['clean_text'].apply(str).apply(lambda x: decontract(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of words in the target text\nclasses = ['Food', 'Movies', 'Clothing']\nfig, arr = plt.subplots(1, 3, figsize = (18,5))\n\ncolor= ['blue', 'green', 'red']\n\nfor i in range(3):\n\n    multiclass_lstm[multiclass_lstm['Originally'] == classes[i]]['clean_text'].str.split().str.len().plot.hist(ax = arr[i], color = color[i])\n    arr[i].set_title(classes[i], color = color[i]);\n    \nfig.suptitle('Distrfibution of Number of words in cleaned text', fontsize = 18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- About 140000 food reviews have about 100 - 200 words\n- Abiut 40000 movies reviews have about 100 words"},{"metadata":{},"cell_type":"markdown","source":"We find the longest sentence to determine our max length "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Max Length\n# calculating length of the longest text to help in sequence padding\n\nmulticlass_lstm['review_len'] = multiclass_lstm['clean_text'].apply(len)\nprint(multiclass_lstm['review_len'].describe())\n\nprint('\\n\\nThe Longest sentence has Maximum length: ', max(multiclass_lstm['review_len']))\n\n#Checkout the mean review length","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multiclass_lstm[multiclass_lstm['clean_text'].isnull()].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting dataset into training, validation and testing set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# fix random seed for reproducibility\nnp.random.seed(7)\n\n#Splitting into training and testing set in the ration 80:20\n\ntrain_DF, test_DF = train_test_split(multiclass_lstm, test_size = 0.2, random_state = 42)  #We get the testing set\n\n\nprint(\"Training data size : \", train_DF.shape)\nprint(\"Test data size : \", test_DF.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To avoid contamination, we will output the current testing set so it is never touched\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_df.to_csv('my_test_file.csv',index=False)\n# train_df.to_csv(\"my_train_set.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the train dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport re\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading the train dataset\ntrain_df = pd.read_csv(\"../input/multiclass-train/my_train_set.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sub_num(text):\n    #substitutinging numbers\n    text = re.sub('[0-9]+', 'numbr', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['clean_text'] = train_df['clean_text'].apply(str).apply(lambda x: sub_num(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['review_len'].describe()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}